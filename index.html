<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" href="style.css">
  <title>My Website</title>
</head>

<body>
  <!-- Header -->
  <section id="header">
    <div class="header container">
      <div class="nav-bar">
        <div class="brand">
          <a href="#hero">
            <h1><span>P</span>remanth <span>C</span>haran</h1>
          </a>
        </div>
        <div class="nav-list">
          <div class="hamburger">
            <div class="bar"></div>
          </div>
          <ul>
            <li><a href="#hero" data-after="Home">Home</a></li>
            <li><a href="#services" data-after="Service">Introduction</a></li>
            <li><a href="#projects" data-after="Projects">Models</a></li>
            <li><a href="#about" data-after="About">Results</a></li>
            <li><a href="#contact" data-after="Contact">Contact</a></li>
          </ul>
        </div>
      </div>
    </div>
  </section>
  <!-- End Header -->


  <!-- Hero Section  -->
  <section id="hero">
    <div class="hero container">
      <div>
        <h1>Hello, This is a Group Project<span></span></h1>
        <h1>DATA 606: CAPSTONE Project <span></span></h1>
        <h1>Automated Image Captioning Using Generative AI: A Transformer-Based Approach <span></span></h1>
        <a href="#projects" type="button" class="cta">Explore</a>
      </div>
    </div>
  </section>
  <!-- End Hero Section  -->

  <!-- Service Section -->
  <section id="services">
    <div class="services container">
      <div class="service-top">
        <h1 class="section-title">Intro<span>D</span>uction</h1>
        <p>Image captioning is a challenging problem that involves generating human-like descriptions for images. By utilizing Vision Transformers, this project aims to achieve improved image understanding and caption generation. The combination of computer vision and Transformers has shown promising results in various natural language processing tasks, and this project explores their application to image captioning.</p>
      </div>
      <div class="service-bottom">
        <div class="service-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/services.png" /></div>
          <h2>Dataset</h2>
          <p>The COCO (Common Objects in Context) dataset is widely used for training and evaluating image captioning models, including Transformer-based architectures, due to its unique features and advantages
          </p>
        </div>
        <div class="service-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/services.png" /></div>
          <h2>Usage</h2>
          <p>
            Ensure you have installed the required dependencies.
            Prepare your dataset in the appropriate format and save it in the project directory.
            Modify the code to load and preprocess your dataset.
            Train the Vision Transformer model using the provided scripts or adapt them to your specific requirements.
            Evaluate the trained model and generate captions for test images.
            Explore and experiment with different model configurations and hyperparameters to improve performance.
          </p>
        </div>
        <div class="service-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/services.png" /></div>
          <h2>Methods</h2>
          <p>
            Vision Transformers (ViTs)
            Attention mechanisms
            Language modeling
            Transfer learning
            Evaluation metrics for image captioning (e.g., BLEU, METEOR, CIDEr)
          </p>
        </div>
        <div class="service-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/services.png" /></div>
          <h2>Technologies</h2>
          <p>
            PyTorch
            Transformers
            TorchVision
            NumPy
            NLTK
            Matplotlib
          </p>
        </div>
      </div>
    </div>
  </section>
  <!-- End Service Section -->

  <!-- Projects Section -->
  <section id="projects">
    <div class="projects container">
      <div class="projects-header">
        <h1 class="section-title">Model<span>Architecture</span></h1>
      </div>
      <div class="all-projects">
        <div class="project-item">
          <div class="project-info">
            <h1>CNN Encoder</h1>
            <p>An InceptionV3 model (pre-trained on ImageNet) is used to process images and extract features, which serve as input to the transformer.</p>
          </div>
          <div class="project-img">
            <img src="./img/img-1.png" alt="img">
          </div>
        </div>
        <div class="project-item">
          <div class="project-info">
            <h1>Transformer Encoder Layer</h1>
            
            <p>	A TransformerEncoderLayer with multi-head self-attention and normalization layers learns the relationships between image features.</p>
          </div>
          <div class="project-img">
            <img src="./img/img-1.png" alt="img">
          </div>
        </div>
        <div class="project-item">
          <div class="project-info">
            <h1>Embeddings Layer</h1>
           
            <p>This layer adds positional embeddings, allowing the model to capture the order of words in captions.</p>
          </div>
          <div class="project-img">
            <img src="./img/img-1.png" alt="img">
          </div>
        </div>
        <div class="project-item">
          <div class="project-info">
            <h1>Transformer Decoder Layer</h1>
           
            <p>The TransformerDecoderLayer generates captions. It includes multi-head attention, feedforward neural networks, and dropout to prevent overfitting. Masking ensures that tokens don’t “see” future tokens when predicting the next word.
            </p>
          </div>
          <div class="project-img">
            <img src="./img/img-1.png" alt="img">
          </div>
        </div>
        <div class="project-item">
          <div class="project-info">
            <h1>GPT2</h1>
           
            <p>	1.	Image Feature Extraction: Use a CNN (e.g., ResNet) to extract features from the image.
              2.	Text Generation: Feed the extracted features into GPT-2, which generates a caption based on the image content.
            </p>
          </div>
          <div class="project-img">
            <img src="./img/img-1.png" alt="img">
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Projects Section -->

  <!-- About Section -->
  <section id="about">
    <div class="about container">
      <div class="col-left">
        <div class="about-img">
          <img src="./img/img-2.png" alt="img">
        </div>
      </div>
      <div class="col-right">
        <h1 class="section-title">Results</h1>
        <p>Github Repository: https://github.com/CapstoneProjectimagecaptioning/image_captioning_transformer?tab=readme-ov-file#6-results-and-analysis</p>
        <p>Project Output: https://huggingface.co/spaces/premanthcharan/Image_Captioining_GenAI</p>
       
      </div>
    </div>
  </section>
  <!-- End About Section -->

  <!-- Contact Section -->
  <section id="contact">
    <div class="contact container">
      <div>
        <h1 class="section-title">Contact <span>info</span></h1>
      </div>
      <div class="contact-items">
        <div class="contact-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/phone.png" /></div>
          <div class="contact-info">
            <h1>Phone</h1>
            <h2>+1 667-419-0143</h2>
            <h2>+1 443-220-2903</h2>
          </div>
        </div>
        <div class="contact-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/new-post.png" /></div>
          <div class="contact-info">
            <h1>Email</h1>
            <h2>palahar1@umbc.edu</h2>
            <h2>charang2@umbc.edu</h2>
          </div>
        </div>
        <div class="contact-item">
          <div class="icon"><img src="https://img.icons8.com/bubbles/100/000000/map-marker.png" /></div>
          <div class="contact-info">
            <h1>Address</h1>
            <h2>UMBC, Baltimore, Maryland</h2>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End Contact Section -->

  <!-- Footer -->
  <section id="footer">
    <div class="footer container">
      <div class="brand">
        <h1><span>P</span>remanth <span>C</span>haran</h1>
      </div>
      <h2>Data 606: Capstone Project</h2>
      <div class="social-icon">
        <div class="social-item">
          <a href="#"><img src="https://img.icons8.com/bubbles/100/000000/facebook-new.png" /></a>
        </div>
        <div class="social-item">
          <a href="#"><img src="https://img.icons8.com/bubbles/100/000000/instagram-new.png" /></a>
        </div>
        <div class="social-item">
          <a href="#"><img src="https://img.icons8.com/bubbles/100/000000/twitter.png" /></a>
        </div>
        <div class="social-item">
          <a href="#"><img src="https://img.icons8.com/bubbles/100/000000/behance.png" /></a>
        </div>
      </div>
      <p>Copyright © 2024. All rights reserved </p>
    </div>
  </section>
  <!-- End Footer -->
  <script src="./app.js"></script>
</body>

</html>